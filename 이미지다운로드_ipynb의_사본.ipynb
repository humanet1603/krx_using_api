{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "12d5En_Bm5Ad9THTtQDcFDX8Z57SKltO1",
      "authorship_tag": "ABX9TyMV6Mz9PEuV/N2L6PQLQlUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/humanet1603/krx_using_api/blob/main/%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4\n",
        "!pip install lxml"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m8U-XUYSVmUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from urllib.parse import urljoin  # URL을 절대 경로로 변환하기 위한 라이브러리\n",
        "\n",
        "# 사용자 에이전트 설정\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
        "}\n",
        "\n",
        "# 검색 URL 설정\n",
        "search_url = \"https://commons.wikimedia.org/w/index.php?search=les+travailleurs+de+la+mer&title=Special:MediaSearch&go=Go&type=image\"\n",
        "\n",
        "# HTTP GET 요청 보내기\n",
        "response = requests.get(search_url, headers=headers)\n",
        "response.raise_for_status()  # 요청에 실패하면 예외 발생\n",
        "\n",
        "# BeautifulSoup를 사용하여 HTML 파싱\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# 이미지 저장 디렉토리 설정\n",
        "save_dir = '/content/drive/MyDrive/바다의 노동자/이미지'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 이미지 URL 추출 및 다운로드\n",
        "image_tags = soup.find_all('img')\n",
        "\n",
        "for img in image_tags:\n",
        "    img_url = img['src']\n",
        "\n",
        "    # 상대 URL을 절대 URL로 변환\n",
        "    img_url = urljoin(search_url, img_url)\n",
        "\n",
        "    # 이미지 URL이 \"upload.wikimedia.org\"로 시작하는지 확인\n",
        "    if \"upload.wikimedia.org\" not in img_url:\n",
        "        continue  # 이미지가 아니면 스킵\n",
        "\n",
        "    # 썸네일 URL을 원본 이미지 URL로 변환\n",
        "    img_url = img_url.replace('/thumb', '').rsplit('/', 1)[0]\n",
        "\n",
        "    # 이미지 파일명 추출\n",
        "    img_name = os.path.basename(img_url)\n",
        "\n",
        "    # 이미지 다운로드\n",
        "    img_response = requests.get(img_url, headers=headers)  # 사용자 에이전트를 포함한 요청\n",
        "    img_response.raise_for_status()  # 이미지 요청에 실패하면 예외 발생\n",
        "\n",
        "    # 지정된 디렉토리에 이미지 파일로 저장\n",
        "    img_path = os.path.join(save_dir, img_name)\n",
        "    with open(img_path, 'wb') as img_file:\n",
        "        img_file.write(img_response.content)\n",
        "\n",
        "    print(f\"Downloaded {img_name} from {img_url}\")\n",
        "\n",
        "print(\"모든 이미지 다운로드가 완료되었습니다.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rkCqnMClW-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csGysmPqXlUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from urllib.parse import urljoin\n",
        "from zipfile import ZipFile  # ZIP 파일 생성을 위한 라이브러리\n",
        "\n",
        "# 사용자 에이전트 설정\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
        "}\n",
        "\n",
        "# 검색 URL 설정\n",
        "search_url = \"https://commons.wikimedia.org/w/index.php?search=les+travailleurs+de+la+mer&title=Special:MediaSearch&go=Go&type=image\"\n",
        "\n",
        "# HTTP GET 요청 보내기\n",
        "response = requests.get(search_url, headers=headers)\n",
        "response.raise_for_status()  # 요청에 실패하면 예외 발생\n",
        "\n",
        "# BeautifulSoup를 사용하여 HTML 파싱\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# 이미지 저장 디렉토리 설정\n",
        "save_dir = '/content/drive/MyDrive/바다의 노동자/이미지'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 이미지 URL 추출 및 다운로드\n",
        "image_tags = soup.find_all('img')\n",
        "downloaded_files = []\n",
        "\n",
        "for img in image_tags:\n",
        "    img_url = img['src']\n",
        "\n",
        "    # 상대 URL을 절대 URL로 변환\n",
        "    img_url = urljoin(search_url, img_url)\n",
        "\n",
        "    # 이미지 URL이 \"upload.wikimedia.org\"로 시작하는지 확인\n",
        "    if \"upload.wikimedia.org\" not in img_url:\n",
        "        continue  # 이미지가 아니면 스킵\n",
        "\n",
        "    # 썸네일 URL을 원본 이미지 URL로 변환\n",
        "    img_url = img_url.replace('/thumb', '').rsplit('/', 1)[0]\n",
        "\n",
        "    # 이미지 파일명 추출\n",
        "    img_name = os.path.basename(img_url)\n",
        "\n",
        "    # 이미지 다운로드\n",
        "    img_response = requests.get(img_url, headers=headers)  # 사용자 에이전트를 포함한 요청\n",
        "    img_response.raise_for_status()  # 이미지 요청에 실패하면 예외 발생\n",
        "\n",
        "    # 지정된 디렉토리에 이미지 파일로 저장\n",
        "    img_path = os.path.join(save_dir, img_name)\n",
        "    with open(img_path, 'wb') as img_file:\n",
        "        img_file.write(img_response.content)\n",
        "\n",
        "    print(f\"Downloaded {img_name} from {img_url}\")\n",
        "    downloaded_files.append(img_path)\n",
        "\n",
        "print(\"모든 이미지 다운로드가 완료되었습니다.\")\n",
        "\n",
        "# ZIP 파일 생성\n",
        "zip_filename = '/content/drive/MyDrive/바다의 노동자/이미지_압축파일.zip'\n",
        "with ZipFile(zip_filename, 'w') as zipf:\n",
        "    for file in downloaded_files:\n",
        "        zipf.write(file, os.path.basename(file))  # ZIP 파일에 파일 추가\n",
        "\n",
        "print(f\"ZIP 파일이 생성되었습니다: {zip_filename}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7op37HNjXrJ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}